{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9087,
     "status": "ok",
     "timestamp": 1584254445575,
     "user": {
      "displayName": "Sahaj d",
      "photoUrl": "",
      "userId": "13950582096679244504"
     },
     "user_tz": -330
    },
    "id": "obhp5UW04aIj",
    "outputId": "34f929a8-83d5-40ce-c119-368fc56712dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Vocabulary.ipynb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import import_ipynb\n",
    "from Vocabulary import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWwEVap557Yp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qd73omQ_4aIs"
   },
   "outputs": [],
   "source": [
    "CONTEXT_LENGTH = 48\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "STEPS_PER_EPOCH = 72000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpQbyUnq4aIy"
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_img(img_path, image_size):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (image_size, image_size))\n",
    "    img = img.astype('float32')\n",
    "    img /= 255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGToYWNI4aI4"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "        self.output_size = None\n",
    "\n",
    "        self.ids = []\n",
    "        self.input_images = []\n",
    "        self.partial_sequences = []\n",
    "        self.next_words = []\n",
    "\n",
    "        self.voc = Vocabulary()\n",
    "        self.size = 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_paths_only(path):\n",
    "        print(\"Parsing data...\")\n",
    "        gui_paths = []\n",
    "        img_paths = []\n",
    "        for f in os.listdir(path):\n",
    "            if f.find(\".gui\") != -1:\n",
    "                path_gui = \"{}/{}\".format(path, f)\n",
    "                gui_paths.append(path_gui)\n",
    "                file_name = f[:f.find(\".gui\")]\n",
    "\n",
    "                if os.path.isfile(\"{}/{}.png\".format(path, file_name)):\n",
    "                    path_img = \"{}/{}.png\".format(path, file_name)\n",
    "                    img_paths.append(path_img)\n",
    "                elif os.path.isfile(\"{}/{}.npz\".format(path, file_name)):\n",
    "                    path_img = \"{}/{}.npz\".format(path, file_name)\n",
    "                    img_paths.append(path_img)\n",
    "\n",
    "        assert len(gui_paths) == len(img_paths)\n",
    "        return gui_paths, img_paths\n",
    "    \n",
    "    def load(self, path, generate_binary_sequences=False):\n",
    "        print(\"Loading data...\")\n",
    "        for f in os.listdir(path):\n",
    "            if f.find(\".gui\") != -1:\n",
    "                gui = open(\"{}/{}\".format(path, f), 'r')\n",
    "                file_name = f[:f.find(\".gui\")]\n",
    "\n",
    "\n",
    "                if os.path.isfile(\"{}/{}.png\".format(path, file_name)):\n",
    "                    img = get_preprocessed_img(\"{}/{}.png\".format(path, file_name), IMAGE_SIZE)\n",
    "                    self.append(file_name, gui, img)\n",
    "                elif os.path.isfile(\"{}/{}.npz\".format(path, file_name)):\n",
    "                    img = np.load(\"{}/{}.npz\".format(path, file_name))[\"features\"]\n",
    "                    self.append(file_name, gui, img)\n",
    "\n",
    "        print(\"Generating sparse vectors...\")\n",
    "        self.voc.create_binary_representation()\n",
    "        self.next_words = self.sparsify_labels(self.next_words, self.voc)\n",
    "        if generate_binary_sequences:\n",
    "            self.partial_sequences = self.binarize(self.partial_sequences, self.voc)\n",
    "        else:\n",
    "            self.partial_sequences = self.indexify(self.partial_sequences, self.voc)\n",
    "\n",
    "        self.size = len(self.ids)\n",
    "        assert self.size == len(self.input_images) == len(self.partial_sequences) == len(self.next_words)\n",
    "        assert self.voc.size == len(self.voc.vocabulary)\n",
    "\n",
    "        print(\"Dataset size: {}\".format(self.size))\n",
    "        print(\"Vocabulary size: {}\".format(self.voc.size))\n",
    "\n",
    "        self.input_shape = self.input_images[0].shape\n",
    "        self.output_size = self.voc.size\n",
    "\n",
    "        print(\"Input shape: {}\".format(self.input_shape))\n",
    "        print(\"Output size: {}\".format(self.output_size))\n",
    "    \n",
    "    def append(self, sample_id, gui, img):\n",
    "        token_sequence = [START_TOKEN]\n",
    "        for line in gui:\n",
    "            line = line.replace(\",\", \" ,\").replace(\"\\n\", \" \\n\")\n",
    "            tokens = line.split(\" \")\n",
    "            for token in tokens:\n",
    "                self.voc.append(token)\n",
    "                token_sequence.append(token)\n",
    "        token_sequence.append(END_TOKEN)\n",
    "\n",
    "        suffix = [PLACEHOLDER] * CONTEXT_LENGTH\n",
    "\n",
    "        a = np.concatenate([suffix, token_sequence])\n",
    "        for j in range(0, len(a) - CONTEXT_LENGTH):\n",
    "            context = a[j:j + CONTEXT_LENGTH]\n",
    "            label = a[j + CONTEXT_LENGTH]\n",
    "\n",
    "            self.ids.append(sample_id)\n",
    "            self.input_images.append(img)\n",
    "            self.partial_sequences.append(context)\n",
    "            self.next_words.append(label)\n",
    "            \n",
    "    def convert_arrays(self):\n",
    "        print(\"Convert arrays...\")\n",
    "        self.input_images = np.array(self.input_images)\n",
    "        self.partial_sequences = np.array(self.partial_sequences)\n",
    "        self.next_words = np.array(self.next_words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def indexify(partial_sequences, voc):\n",
    "        temp = []\n",
    "        for sequence in partial_sequences:\n",
    "            sparse_vectors_sequence = []\n",
    "            for token in sequence:\n",
    "                sparse_vectors_sequence.append(voc.vocabulary[token])\n",
    "            temp.append(np.array(sparse_vectors_sequence))\n",
    "\n",
    "        return temp\n",
    "\n",
    "    @staticmethod\n",
    "    def binarize(partial_sequences, voc):\n",
    "        temp = []\n",
    "        for sequence in partial_sequences:\n",
    "            sparse_vectors_sequence = []\n",
    "            for token in sequence:\n",
    "                sparse_vectors_sequence.append(voc.binary_vocabulary[token])\n",
    "            temp.append(np.array(sparse_vectors_sequence))\n",
    "\n",
    "        return temp\n",
    "\n",
    "    @staticmethod\n",
    "    def sparsify_labels(next_words, voc):\n",
    "        temp = []\n",
    "        for label in next_words:\n",
    "            temp.append(voc.binary_vocabulary[label])\n",
    "\n",
    "        return temp\n",
    "\n",
    "    def save_metadata(self, path):\n",
    "            np.save(\"{}/meta_dataset\".format(path), np.array([self.input_shape, self.output_size, self.size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VKZBu57b4aI-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
