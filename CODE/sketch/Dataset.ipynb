{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ImagePreprocessor.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pdb\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import import_ipynb\n",
    "from ImagePreprocessor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_FILE              = 'model/vocabulary.vocab'\n",
    "TRAINING_SET_NAME       = \"training_set\"\n",
    "VALIDATION_SET_NAME     = \"validation_set\"\n",
    "BATCH_SIZE              = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\n",
    "    def __init__(self, data_input_folder, test_set_folder=None):\n",
    "        self.data_input_folder = data_input_folder\n",
    "        self.test_set_folder   = test_set_folder\n",
    "\n",
    "    def split_datasets(self, validation_split):\n",
    "        sample_ids = self.populate_sample_ids()\n",
    "        print(\"Total number of samples: \", len(sample_ids))\n",
    "\n",
    "        train_set_ids, val_set_ids, shuffled_sampled_ids = self.get_all_id_sets(validation_split, sample_ids)\n",
    "        training_path, validation_path = self.split_samples(train_set_ids, val_set_ids)\n",
    "\n",
    "        return training_path, validation_path\n",
    "\n",
    "    def split_samples(self, train_set_ids, val_set_ids):\n",
    "        training_path, validation_path = self.create_data_folders()\n",
    "        self.copy_files_to_folders(train_set_ids, training_path)\n",
    "        self.copy_files_to_folders(val_set_ids, validation_path)\n",
    "        return training_path, validation_path\n",
    "\n",
    "    def preprocess_data(self, training_path, validation_path, augment_training_data):\n",
    "        train_img_preprocessor = ImagePreprocessor()\n",
    "        train_img_preprocessor.build_image_dataset(training_path, augment_data=augment_training_data)\n",
    "        val_img_preprocessor = ImagePreprocessor()\n",
    "        val_img_preprocessor.build_image_dataset(validation_path, augment_data=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##########################################\n",
    "    ####### PRIVATE METHODS ##################\n",
    "    ##########################################\n",
    "\n",
    "    @classmethod\n",
    "    def load_vocab(cls):\n",
    "        file = open(VOCAB_FILE, 'r')\n",
    "        text = file.read().splitlines()[0]\n",
    "        file.close()\n",
    "        tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
    "        tokenizer.fit_on_texts([text])\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "        return tokenizer, vocab_size\n",
    "\n",
    "    @classmethod\n",
    "    def create_generator(cls, data_input_path, max_sequences):\n",
    "        img_features, text_features = Dataset.load_data(data_input_path)\n",
    "        total_sequences = 0\n",
    "        for text_set in text_features: total_sequences += len(text_set.split())\n",
    "        steps_per_epoch = total_sequences // BATCH_SIZE\n",
    "        tokenizer, vocab_size = Dataset.load_vocab()\n",
    "        data_gen = Dataset.data_generator(text_features, img_features, max_sequences, tokenizer, vocab_size)\n",
    "        return data_gen, steps_per_epoch\n",
    "\n",
    "    @classmethod\n",
    "    def data_generator(cls, text_features, img_features, max_sequences, tokenizer, vocab_size):\n",
    "        while 1:\n",
    "            for i in range(0, len(text_features), 1):\n",
    "                Ximages, XSeq, y = list(), list(),list()\n",
    "                for j in range(i, min(len(text_features), i+1)):\n",
    "                    image = img_features[j]\n",
    "                    desc = text_features[j]\n",
    "                    in_img, in_seq, out_word = Dataset.process_data_for_generator([desc], [image], max_sequences, tokenizer, vocab_size)\n",
    "                    for k in range(len(in_img)):\n",
    "                        Ximages.append(in_img[k])\n",
    "                        XSeq.append(in_seq[k])\n",
    "                        y.append(out_word[k])\n",
    "                yield [[np.array(Ximages), np.array(XSeq)], np.array(y)]\n",
    "\n",
    "    @classmethod\n",
    "    def process_data_for_generator(cls, texts, features, max_sequences, tokenizer, vocab_size):\n",
    "        X, y, image_data = list(), list(), list()\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        for img_no, seq in enumerate(sequences):\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_sequences)[0]\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                image_data.append(features[img_no])\n",
    "                X.append(in_seq[-48:])\n",
    "                y.append(out_seq)\n",
    "        return np.array(image_data), np.array(X), np.array(y)\n",
    "\n",
    "    @classmethod\n",
    "    def load_data(cls, data_input_path):\n",
    "        text = []\n",
    "        images = []\n",
    "        all_filenames = os.listdir(data_input_path)\n",
    "        all_filenames.sort()\n",
    "        for filename in all_filenames:\n",
    "            if filename[-3:] == \"npz\":\n",
    "                image = np.load(data_input_path+'/'+filename)\n",
    "                images.append(image['features'])\n",
    "            elif filename[-3:] == 'gui':\n",
    "                file = open(data_input_path+'/'+filename, 'r')\n",
    "                texts = file.read()\n",
    "                file.close()\n",
    "                syntax = '<START> ' + texts + ' <END>'\n",
    "                syntax = ' '.join(syntax.split())\n",
    "                syntax = syntax.replace(',', ' ,')\n",
    "                text.append(syntax)\n",
    "        images = np.array(images, dtype=float)\n",
    "        return images, text\n",
    "\n",
    "    def create_data_folders(self):\n",
    "        training_path = \"{}/{}\".format(os.path.dirname(self.data_input_folder), TRAINING_SET_NAME)\n",
    "        validation_path = \"{}/{}\".format(os.path.dirname(self.data_input_folder), VALIDATION_SET_NAME)\n",
    "\n",
    "        self.delete_existing_folders(training_path)\n",
    "        self.delete_existing_folders(validation_path)\n",
    "\n",
    "        if not os.path.exists(training_path): os.makedirs(training_path)\n",
    "        if not os.path.exists(validation_path): os.makedirs(validation_path)\n",
    "        return training_path, validation_path\n",
    "\n",
    "    def copy_files_to_folders(self, sample_ids, output_folder):\n",
    "        copied_count = 0\n",
    "        for sample_id in sample_ids:\n",
    "            sample_id_png_path = \"{}/{}.png\".format(self.data_input_folder, sample_id)\n",
    "            sample_id_gui_path = \"{}/{}.gui\".format(self.data_input_folder, sample_id)\n",
    "            if os.path.exists(sample_id_png_path) and os.path.exists(sample_id_gui_path):\n",
    "                output_png_path = \"{}/{}.png\".format(output_folder, sample_id)\n",
    "                output_gui_path = \"{}/{}.gui\".format(output_folder, sample_id)\n",
    "                shutil.copyfile(sample_id_png_path, output_png_path)\n",
    "                shutil.copyfile(sample_id_gui_path, output_gui_path)\n",
    "                copied_count += 1\n",
    "        print(\"Moved {} files from {} to {}\".format(copied_count, self.data_input_folder, output_folder))\n",
    "\n",
    "    def delete_existing_folders(self, folder_to_delete):\n",
    "        if os.path.exists(folder_to_delete):\n",
    "            shutil.rmtree(folder_to_delete)\n",
    "            print(\"Deleted existing folder: {}\".format(folder_to_delete))\n",
    "\n",
    "    def populate_sample_ids(self):\n",
    "        all_sample_ids = []\n",
    "        full_path = os.path.realpath(self.data_input_folder)\n",
    "        for f in os.listdir(full_path):\n",
    "            if f.find(\".gui\") != -1:\n",
    "                file_name = f[:f.find(\".gui\")]\n",
    "                if os.path.isfile(\"{}/{}.png\".format(self.data_input_folder, file_name)):\n",
    "                    all_sample_ids.append(file_name)\n",
    "        return all_sample_ids\n",
    "\n",
    "    def get_all_id_sets(self, validation_split, sample_ids):\n",
    "        np.random.shuffle(sample_ids)\n",
    "        val_count = int(validation_split * len(sample_ids))\n",
    "        train_count = len(sample_ids) - val_count\n",
    "        print(\"Splitting datasets, training samples: {}, validation samples: {}\".format(train_count, val_count))\n",
    "        train_set, val_set = self.split_paths(sample_ids, train_count, val_count)\n",
    "\n",
    "        return train_set, val_set, sample_ids\n",
    "\n",
    "    def split_paths(self, sample_ids, train_count, val_count):\n",
    "        train_set = []\n",
    "        val_set = []\n",
    "        hashes = []\n",
    "        for sample_id in sample_ids:\n",
    "            f = open(\"{}/{}.gui\".format(self.data_input_folder, sample_id), 'r', encoding='utf-8')\n",
    "\n",
    "            with f:\n",
    "                chars = \"\"\n",
    "                for line in f:\n",
    "                    chars += line\n",
    "                content_hash = chars.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "                content_hash = hashlib.sha256(content_hash.encode('utf-8')).hexdigest()\n",
    "\n",
    "                if len(val_set) == val_count:\n",
    "                    train_set.append(sample_id)\n",
    "                else:\n",
    "                    is_unique = True\n",
    "                    for h in hashes:\n",
    "                        if h is content_hash:\n",
    "                            is_unique = False\n",
    "                            break\n",
    "\n",
    "                    if is_unique:\n",
    "                        val_set.append(sample_id)\n",
    "                    else:\n",
    "                        train_set.append(sample_id)\n",
    "\n",
    "                hashes.append(content_hash)\n",
    "\n",
    "        assert len(val_set) == val_count\n",
    "\n",
    "        return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
